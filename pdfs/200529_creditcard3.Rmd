---
title: "Model Development for Prediction of Creditcard Fraud"
author: "Jason Schmidberger"
date: "29/05/2020"
output:
  pdf_document: default
toc: true
toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pdf.options(useDingbats = TRUE)
```

```{r Libraries, include=F}
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
if(!require(Rborist)) install.packages("Rborist", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(DMwR)) install.packages("DMwR", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(dMod)) install.packages("dMod", repos = "http://cran.us.r-project.org")
if(!require(png)) install.packages("png", repos = "http://cran.us.r-project.org")
if(!require(e1071)) install.packages("e1071", repos = "http://cran.us.r-project.org")
```
\newpage
# 1 - Introduction

The use of credit cards is ubiquitous in modern society. Card based 'cashless' transations are a mainstay of everyday life. However, with this shift in lifestyle, comes new risks. It is a largely faceless process, and prone to misuse. Credit card fraud is an international problem, and is the focus of much effort to detect it. This body of work deals with the detection of credit card fraud using a dataset taken from the following website; https://www.kaggle.com/mlg-ulb/creditcardfraud.

It is a dataset relating to creditcard transactions in September of 2013 by European cardholders. The data is all numeric and the majority of descriptors (V1 to V28) are numerical values that resulted from a Principle Component Analysis (PCA) of confidential data not made available as part of the dataset. Aside from these, 'Time' is the time that has elapsed between transactions, and 'Amount' is the amount of money involved. "Class' is the response variable and is either a 1 (fraudulent transaction) or a 0 (non-fraudulent). 

This is an historically much studied dataset, and a commonly used example to showcase an imbalanced dataset. As I undertook this work, I have learnt much and in an effort to convey to the reader that this is my own work I am including a lot of the fundamental work, chronologically. For example, I was not aware at first that it was an imbalanced dataset until I had already performed some basic analysis. It thereby reads much like a work of personal discovery which should illustrate my understanding better than were I to just stream line a model and present it. 

In this work I begin by downloading the creditcard dataset (hosted on my Dropbox account), divide it into training (cc_train) and test (cc_test) subsets. The testing dataset is kept until the end for validation of the final model. The training dataset is subdivided again for training (cc_train_wk) and cross validation (cc_train_cv) purposes. I establish a linear regression model as a benchmark for subsequent model comparison. I then apply random forest algorithm (Rborist) to try to improve on the benchmark model. This leads me to address the imbalanced nature of the dataset, employing the SMOTE algorithm to balance out the data. Parallel to the SMOTE work, a Cost Sensitive Learning (CSL) metric is investigated for its ability to focus in on the intent to minimise false negatives as much as possible. At the expense (to some degree) of false positives. The final model improves on the benchmark model when applied to cc_train_cv, and also when applied to cc_test.   


# 2 - Methods

## 2.1 - Downloading and loading dataset
Thought the original dataset was taken from Kaggle, I have hosted it on my own dropbox account in order to enable the reader to access it using this code, and not have to sign up to Kaggle themselves. The following code should download, unzip and read in a dataset called "ccds" that has 284807 rows and 31 columns. 

```{r Downloading data}
dl <- tempfile()
download.file("https://www.dropbox.com/s/rg6r68kdyual1wv/creditcard.csv.zip?dl=1", dl)
ccds <- read.csv(unzip(dl, "creditcard.csv"))
dim(ccds) 
```


## 2.2 - Understanding the dataset
To get a better feel for the data, I will conduct a series of analyses. 

Firstly, checking for any NA entries in the dataset. 

```{r}
sapply(ccds, function(x) sum(is.na(x)))
```

The dataset seems complete, and free and any NA entries. 

You can see from the output of the NA analysis above, there are 31 columns in this dataset, with the last (31 - Class) being the response variable. Columns 2 to 29 are the principle component numerics, and column 1 and 30 are the time and amount of the tranaction respectively.   




### 2.2.1 - Time and Amount. 
Figure 1 is a plot showing the Time vs Amount. In this instance, the Class response variable is still a numeric value, such that it shows up in the key as a continuous scale from 0 to 1. It would make more sense to convert it to a factor for these visualisation purposes. 

It is also clear that fraudulent cases (Class = 1) tend to have lower Amount values associated with them. This would make sense as the fraudster would not want to attract unnecessary attention by making a large transaction. 

```{r Amount vs Time, include=F}
# Visualisation.
tmp1 <- ccds %>% filter(Class == 0)
tmp2 <- ccds %>% filter(Class == 1)
ggplot() + geom_point(data = tmp1, aes(Time, Amount, colour = Class)) + labs(title = "Time/Amount scatterplot") + geom_point(data = tmp2, aes(Time, Amount, colour = Class))
ggsave('temp.png', dpi = 600)
rm(tmp1, tmp2) 
```

![Scatterplot of Time vs Amount](temp.png)

A quick check of the dataset (str(ccds)) actually shows that the Class variable is an integer. While the rest of the variables are numerics (output not shown). The Class variable will be converted to a factor as necessary for clarity in figures below. 

```{r, include=F}
str(ccds)
```


### 2.2.2 - Time difference and Amount. 
The Time variable is the amount of time that has passed from the first trasaction (in seconds). I am introducing a new variable, "diff" that is an expression of the amount of time that has passed since the previous transaction. 

```{r Time to time diff, include=F}
ccds <- ccds %>%
  mutate(diff = Time - lag(Time))
# This process introduced one NA value for the first row, for which there is no previous row. This 
# should be converted to 0 for simplicity. 
ccds[is.na(ccds)] <- 0
tmp1 <- ccds
tmp1$Class <- as.factor(tmp1$Class)
tmp1$ID <- seq.int(nrow(tmp1))
tmp1[is.na(tmp1)] <- 0
tmp2 <- tmp1 %>% filter(Class == 0)
tmp3 <- tmp1 %>% filter(Class == 1)
ggplot() + geom_point(data = tmp2, aes(diff, Amount, colour = Class)) + labs(title = "Time Diff/Amount scatterplot") + geom_point(data = tmp3, aes(diff, Amount, colour = Class)) 
ggsave('temp2.png', dpi = 600)
rm(tmp1, tmp2, tmp3)
```

![Scatterplot of 'Time difference from previous transaction' vs Amount](temp2.png)



You can see from the "Time Diff/Amount scatterplot" (Figure 2) that not only is there a trend for fraudulent tranastions to be lower in amount, but also to be closer together in time. Meaning they tend to be grouped closer together like a quick succession of a few small transactions. 

\newpage
### 2.2.3 - Principle Components.
Remembering columns V1 to V28 are the output of a principle component analysis that has had its origins kept secret for privacy reasons, it would be worth exploring how these take shape. 
To start with, I will look at the individual and cumulative contribution of the principle components (PCs) to the dataset variation by calculating the eigenvalues. 

#### 2.2.3.1 - Contribution to Variance

```{r Variance contributions, include=F}
evN <- seq(2,29,1)
# Each column Vx is a Principle Component (PC) of a PCA calculation (not performed here). 
# Calculating the eigenvalues for each PC;
eigen1 <- sapply(evN, function(x) {
  var1 <- ccds[,x] %>% var(.)
})

# Plotting these
ev1 <- eigen1^2/sum(eigen1^2)
ev2 <- cumsum(eigen1^2/sum(eigen1^2))
ev_plot1 <- ggplot() + geom_point(aes(evN, ev1)) + ylim(c(0, 0.3)) + 
  labs(title = "Variance contribution", x = "Principle component", 
                                            y = "Fractional variation")
ev_plot2 <- ggplot() + geom_point(aes(evN, ev2)) + ylim(c(0, 1)) +
  labs(title = "Cumulative Variance", x = "Principle component", 
                                            y = "Fractional variation")

grid.arrange(ev_plot1, ev_plot2, nrow = 1)
```

Looking at the 'Variance contribution' plot (above - left), it is clear the PCs are presented in order of contribution to dataset variance, with PC1 having a value close to 0.3 of the fractional variance. Looking at the 'Cumulative Variance' figure (above - right), more than 50% of the dataset variance as captured by the PCA is encapsulated in the first three PCs. Plotting these against eachother (Figure 3) and also against PC28 (the last and least 'contributing' PC) you can see the spread of the data in each case, across both PCs in each plot, paying special attention to the fraudulent cases (Class = 1). Notably there is minimal spread across the PC28 as would be expected from the least impacting PC. 


```{r PC PC scatterplots, include=F}
tmp1 <- ccds
tmp1$Class <- as.factor(tmp1$Class)
tmp2 <- tmp1 %>% filter(Class == 0)
tmp3 <- tmp1 %>% filter(Class == 1)
pc1_2 <- ggplot() + geom_point(data = tmp2, aes(V1, V2, colour = Class)) + labs(title = "PC1/PC2 scatterplot", x="PC1", y="PC2") + geom_point(data = tmp3, aes(V1, V2, colour = Class)) 
pc1_3 <- ggplot() + geom_point(data = tmp2, aes(V1, V3, colour = Class)) + labs(title = "PC1/PC3 scatterplot", x="PC1", y="PC3") + geom_point(data = tmp3, aes(V1, V3, colour = Class))
pc2_3 <- ggplot() + geom_point(data = tmp2, aes(V2, V3, colour = Class)) + labs(title = "PC2/PC3 scatterplot", x="PC2", y="PC3") + geom_point(data = tmp3, aes(V2, V3, colour = Class))
pc1_28 <- ggplot() + geom_point(data = tmp2, aes(V1, V28, colour = Class)) + labs(title = "PC1/PC28 scatterplot", x="PC1", y="PC28") + geom_point(data = tmp3, aes(V1, V28, colour = Class))
# grid.arrange(pc1_2, pc1_3, pc2_3, pc1_28, nrow = 2)
g1 <- arrangeGrob(pc1_2, pc1_3, pc2_3, pc1_28, nrow = 2) #generates g
ggsave('temp3.png', g1, dpi = 600)
rm(tmp1, tmp2, tmp3)
```

![Scatterplots of Principle component examples](temp3.png)


\newpage
### 2.2.4 - Cross dataset correlation
A solid starting point to establish a prediction model for this dataset should be regression. In the interest of looking into this, the cross dataset correlation is investigated. 

```{r Correlation plot, echo=F}
correlations <- cor(ccds)
corrplot(correlations, method = "circle", title = "Cross correlation chart")
# correlations
```

You can see from the plot above (circles indicate clear correlation) that the identity correlation values running down the diagnal. There is no cross correlation within the PC group, but there is between the PC group and 'Time", 'Amount', 'diff', and most importantly, 'Class'. 

Figure 4 shows a series of four scatterplots looking at two cases of strong correlation (Time/PC3 and PC17/Class) and two cases of weak correlation (Time/PC2 and PC15/Class). 


```{r Time PC and PC Class, include=F}
tmp1 <- ccds
tmp1$Class <- as.factor(tmp1$Class)
tmp2 <- tmp1 %>% filter(Class == 0)
tmp3 <- tmp1 %>% filter(Class == 1)
pcT_3 <- ggplot() + geom_point(data = tmp2, aes(Time, V3, colour = Class)) + labs(title = "Time/PC3 scatterplot", x="Time", y="PC3") + geom_point(data = tmp3, aes(Time, V3, colour = Class)) 
pc17_C <- ggplot() + geom_point(data = tmp2, aes(V17, Class, colour = Class)) + labs(title = "PC17/Class scatterplot", x="PC17", y="Class") + geom_point(data = tmp3, aes(V17, Class, colour = Class))
pcT_2 <- ggplot() + geom_point(data = tmp2, aes(Time, V2, colour = Class)) + labs(title = "Time/PC2 scatterplot", x="Time", y="PC2") + geom_point(data = tmp3, aes(Time, V2, colour = Class)) 
pc15_C <- ggplot() + geom_point(data = tmp2, aes(V15, Class, colour = Class)) + labs(title = "PC15/Class scatterplot", x="PC15", y="Class") + geom_point(data = tmp3, aes(V15, Class, colour = Class))
# grid.arrange(pcT_3, pc17_C, pcT_2, pc15_C, nrow = 2)
g2 <- arrangeGrob(pcT_3, pc17_C, pcT_2, pc15_C, nrow = 2) #generates g
ggsave('temp4.png', g2, dpi = 600)
rm(tmp1, tmp2, tmp3, g2)
```

![Scatterplots of PCs vs Time or Class](temp4.png)


Though it is not possible to know what original descriptors the separate PCs are derived from, it is clear that PC3 has some properties that correlates well with the Time descriptor. In the Time/PC3 plot you can see a succession of four negatively correlated fraud case 'spikes'. The actual correlation for this pair is only -0.42 so overall it is a weak correlation, but there is a clear relationship here. Interestingly it seems to capture a series of runs on fraudelent transactions, all closely spaced together in 'Time', and reported by what ever property PC3 is derived from. This is less clear for the case of the Time/PC2 plot, though these events are still evident, though much less clear, and this time positively correlated. 

Looking at the two Class/PC cases, a different kind of relationship manifests. Clearly this will result in a binary classification based model. The case with a 'strong' (-0.33) correlation, PC17/Class, shows the two classes with a left trend for fraud, and a rightward trend for non-fraud (Class = 0). Obviously there is a lot of overlap, but there is a clear directional shift that can be taken advantage of. Likely using logistic regression in the first case. In the other 'weak' (-0.004) correlation case involving Class, PC15/Class, the non-fraud cases more or less completely overlap the fraud cases, making it very difficult to use PC15 to drive a prediction of fraud.


## 2.4 - Benchmark model.
Having gotten a feel for the dataset, it is time to start looking at a predictive model. A good starting point will be basic regression if for no other reason then to provide a benchmark against which to compare successive models. 

Lets begin by dividing up the dataset into a training set and a test set. The test set should be left aside completely until the final model needs to be assessed. These will be 'cc_train' and 'cc_test', containing 90% and 10% of the data respectively. 

```{r Training and test set split, include = F}
# Time to divide the dataset into training and testing subsets. 90% and 10% respectively. 
set.seed(1, sample.kind = "Rejection")
test_index <- createDataPartition(y = ccds$Class, times = 1, p = 0.1, list = FALSE)
cc_train <- ccds[-test_index,]
cc_test <- ccds[test_index,]
```

The 'cc_train' set will be further divided into working (cc_train_wk), and cross validation (cc_train_cv) subsets for training and testing cycles during model refinement. 

```{r cc_train second split, include=F}
# I need to set up some further data partitions for some basic cross validation. 
set.seed(1, sample.kind = "Rejection")
test_index2 <- createDataPartition(y = cc_train$Class, times = 1, p = 0.1, list = FALSE)
cc_train_wk <- ccds[-test_index2,]
cc_train_cv <- ccds[test_index2,]
```



### 2.4.1 - Linear regression model.
Ignoring for now the idea that this should be a classification problem, I will have a look at basic linear regression. 

```{r linear regression model}
fit_lm <- lm(Class ~ .,data = cc_train_wk)
y_hat_lm <- predict(fit_lm, newdata = cc_train_cv)
mean(y_hat_lm == cc_train_cv$Class)
```

Assessing the mean of cases where the predicted value equals the actual value returns a 0. This is a report of "Accuracy" for the model. A value of 0 is not surprising as this regression model will return predictions between 0 and 1, and the actual values can only be 0 or 1.

Looking instead at a RMSE calculation, the model does return a more reasonable value of 0.027. 

```{r RMSE calculation}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

RMSE(cc_train_cv$Class, y_hat_lm)
```

However, as it is a classification problem, implementation of a threshold cut off is more appropriate. 
A cutoff value can be screened to return the best assessment metric, what ever that should be. To begin with I will use Accuracy. The cutoff screen will look at predicted values between 0 and 0.2, and as they pass the cutoff in each case they are made a value of 1. 

```{r Threshold screen lm}
cut <- seq(0, 0.2, 0.01)

set.seed(1, sample.kind = "Rejection")
fit_lm1 <- sapply(cut, function(x){
  fit_lm <- lm(Class ~ .,data = cc_train_wk)
  y_hat_lm <- predict(fit_lm, newdata = cc_train_cv)
  y_hat_lm <- ifelse(y_hat_lm >= x, 1,0)
  mean(y_hat_lm == cc_train_cv$Class)
})
plot(cut, fit_lm1, main = "Tuning cutoff for lm", ylab = "Accuracy")
```

You see from the plot above the curve quickly levels out, meaning that any predicted value above ~0.025 should be considered a fraudulent event and made Class = 1. 

The maximum accuracy attained using this method is shown below. 

```{r, echo=F}
tibble(max_cut = (cut[which.max(fit_lm1)]), accuracy = max(fit_lm1)) %>% knitr::kable()
```

Following through with this, and selecting the cutoff that returned the best fitting score for mean difference;

```{r lm outcome}
fit_lm <- lm(Class ~ ., data = cc_train_wk)
y_hat_lm <- predict(fit_lm, newdata = cc_train_cv)
y_hat_lm <- ifelse(y_hat_lm >= cut[which.max(fit_lm1)], 1,0)
acc_lm <- mean(y_hat_lm == cc_train_cv$Class)
rmse_lm <- RMSE(cc_train_cv$Class, y_hat_lm)
tibble(lm_accuracy = acc_lm, lm_RMSE = rmse_lm) %>% knitr::kable()
```

On face value, this is an excellent accuracy, and a decent RMSE value. A better method to assess a classification problem however is a confusion matrix that lists not only correctly assigned positive (Class = 1) and negative (Class = 0), but also false positive and false negative assignments. 
\newpage
Confusion Matrix

```{r confusiun matrix, echo=F}
lm_tab <- table(y_hat_lm, cc_train_cv$Class)
rownames(lm_tab) <- c("Pred 0", "Pred 1")
colnames(lm_tab) <- c("Actual 0", "Actual 1")
lm_tab %>% knitr::kable()
```

In this table, 25589 instances were correctly predicted as non-fraud, and 37 were correctly predicted as fraud. Furthermore, there were 5 instances that were 'false positives' (i.e. predicted as fraud when they were not), and 2 were 'false negatives' (i.e. predicted as non-fraud when they were in fact cases of fraud). As part of the model assessment we should include 'false positive' and 'false negative' values as these are of particular interest in this problem. A 'false positive' value of 5 is actually 12.8% of all predicted frauds ((5/(5 + 37))*100) that are incorrectly assigned as frauds. Furthermore, a 'false negative' value of 2 is 5.1% of all actual frauds that are being missed by the model. Hense we must try to minimise them. 

```{r, echo=F}
model_results <- tibble(method = "linear regression", Accuracy = acc_lm, RMSE = rmse_lm, 
                       false_pos = lm_tab[2,1], false_neg = lm_tab[1,2])
model_results %>% knitr::kable()
```

### 2.4.1.1 - LM with limited predictors.
Looking to see if I am overtraining the data, I will limit the predictors to only those that had a significant enough correlation with Class (see correlation plot above) to register a circle. This included PC1 to PC7, PC9 to PC12, PC14, and PC16 to PC18. 

```{r, include=F}
fit_lm2 <- lm(Class ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V9 + V10 + V11 +
                V12 + V14 + V15 + V16 + V17 + V18, data = cc_train_wk)
y_hat_lm2 <- predict(fit_lm2, newdata = cc_train_cv)
y_hat_lm2 <- ifelse(y_hat_lm2 >= cut[which.max(fit_lm1)], 1,0)
acc_lm2 <- mean(y_hat_lm2 == cc_train_cv$Class)
rmse_lm2 <- RMSE(cc_train_cv$Class, y_hat_lm2)
lm_tab2 <- table(y_hat_lm2, cc_train_cv$Class)
rownames(lm_tab2) <- c("Pred 0", "Pred 1")
colnames(lm_tab2) <- c("Actual 0", "Actual 1")

model_results <- bind_rows(model_results,
                          data_frame(method = "LM lim Preds",
                                     Accuracy = acc_lm2, RMSE = rmse_lm2,
                                     false_pos = lm_tab2[2,1], false_neg = lm_tab2[1,2]))
```

```{r, echo=F}
model_results %>% knitr::kable()
```


This more focused model acutally performed worse slightly (one more false positive). There is no disadvantage to using all predictors in this case. 



### 2.4.2 - Logistic regression model.
As this is a binary classification system it is worth looking at logistic regression.
Using the same cutoff threshold approach seen above;

```{r Threshold screen logistic regression, include=F}
cutoff <- seq(0, 0.2, 0.01)
set.seed(1, sample.kind = "Rejection")
glm.fit <- glm(Class ~ ., data = cc_train_wk, family = binomial)
logreg_fit <- sapply(cutoff, function(cut){
  y_hat <- predict(glm.fit, newdata = cc_train_cv, type = "response")
  y_hat <- ifelse(y_hat >= cut, 1,0)
  mean(y_hat == cc_train_cv$Class)
})

# Taking the best outcome forward and repeating in formal model. 
y_hat_log <- predict(glm.fit, newdata = cc_train_cv, type = "response")
y_hat_log <- ifelse(y_hat_log >= cutoff[which.max(logreg_fit)], 1,0)
acc_log <- mean(y_hat_log == cc_train_cv$Class)
log_tab <- table(y_hat_log, cc_train_cv$Class)
rmse_log <- RMSE(cc_train_cv$Class, y_hat_log)

model_results <- bind_rows(model_results,
                          data_frame(method = "Logistic Regression",
                                     Accuracy = acc_log, RMSE = rmse_log,
                                     false_pos = log_tab[2,1], false_neg = log_tab[1,2]))
```

```{r, echo=F}
model_results %>% knitr::kable()
```


Logistic regression is notably worse with higher false positive (F_pos) and higher false negative (F_neg) values. This model will not be considered further.  

This is likely the limit of what I can achieve with regression. I was only intending to use it as a benchmark model. I will move onto another approach. 


## 2.5 - Classification model development. 

### 2.5.1 - Random Forests
This kind of classification problem is well suited to modeling using a random forest approach. Given the large dataset I will begin using Rborist as it is better suited to a dataset with a large number of instances, and moderate number of predictors. 

First of all, I will formally change the Class response variables in both cc_train_wk and cc_train_cv to a two level factor (levels = 0, 1).


```{r Changing to factors, include=F}
cc_train_wk$Class <- as.factor(cc_train_wk$Class)
cc_train_cv$Class <- as.factor(cc_train_cv$Class)
```

I spent a bit of time using the caret package to 'train' the model (not reported here for interests of brevity) playing with predFixed and minNode values are these are thought to have the most impact on Rborist model development. PredFixed is the number of predictors to consider for each split in the tree, and minNode is minimum number of distinct instances required to split a node.

This investigation returned the result that minNode = 15, and predFixed = 20 should return the optimal model. This could be fine tuned, but for now given the large computational requirements we will move forward.  

```{r Rborist with predFixed 20 and minNode 15, include=F}
set.seed(1, sample.kind = "Rejection")
rf_1 <- Rborist(cc_train_wk[,c(1:30)], cc_train_wk$Class, predFixed = 20, minNode = 15)
rf_pred1 <- predict(rf_1, newdata = cc_train_cv[,c(1:30)])
acc_rf1 <- mean(rf_pred1$yPred == cc_train_cv$Class)
rf_tab1 <- table(rf_pred1$yPred, cc_train_cv$Class)
rmse_rf1 <- RMSE(cc_train_cv$Class, rf_pred1$yPred)
model_results <- bind_rows(model_results,
                          data_frame(method = "RF Rborist",
                                     Accuracy = acc_rf1, RMSE = rmse_rf1,
                                     false_pos = rf_tab1[2,1], false_neg = rf_tab1[1,2]))
```

```{r, echo=F}
model_results %>% knitr::kable()
```

To start with, RMSE will no longer be considered as it has no relevance to factors and classification. Looking at the Rborist results, it looks promising as accuracy is better than seen for the regression methods, and even the 'false positive' results are down to just 1 wrong prediction of fraud. BUT, the number of 'false_negative values is the same as what I got with regression. I need to be able to reduce this value. 

This brings us to an important revelation about this credit card fraud problem. The value of false positives and false negatives to the bank may not be equal. It is highly likely that false negatives (true frauds that have been missed by the model) are a much bigger deal than false positives. I will talk more on this below, but before this can be discussed I need to introduce another important issue with this dataset. 

\newpage
## 2.6 - Imbalanced datasets
Upon closer inspection it is clear to see that this credit card dataset is an imbalanced dataset with regard to the response variable. Imbalanced classifications pose a problem for predictive modelling as most machine learning algorithms used for classification were designed using the assumption that the classes have roughly equal representation. See this site for more details;
(https://machinelearningmastery.com/what-is-imbalanced-classification/)

Importantly, caution must be taken when evaluating the effectiveness of classification models in
cases where data imbalanced. A simple measurement of accuracy may not be the best metric for 
model evaluation. 

If we look at the proportion of instances in the original ccds dataset that are either non-fraudulent (Class = 0) or fraudulent (Class = 1) we see a striking result. 

```{r CCDS Class proportions, echo=F}
prop.table(table(ccds$Class)) %>% knitr::kable()
```

From this table it is clear that there is a big imbalance in the Class instances with only 0.17% of the data being fraudulent (i.e. Class = 1). This is not necessarily surprising as most transactions should be non-fraudulent otherwise something has gone wrong with the system. It is in fact why many fraudsters are able to get away with it as they are able to hide in the masses of transactions that take place every day. And this is why finding machine learning algorithms that are able to find them in is so important. 

There are a number of approaches that can be used when dealing with imbalanced data. Common among them is to modify the dataset itself in some way to balance out the difference in classes. This could involve a process called undersampling, where the majority class (Class = 0 in this case) is under sampled to generate a new dataset that is either completety balanced in classes or partially so, depending on how stringent you choose to be in the undersampling. An obvious alternative is oversampleing. This is where the minority class (Class = 1 here) is oversampled to drive it towards equality with the incidence count of the majority class. The extra samples could come from random replication of real instances, or by more complicated methods that aim to create new instances through some relationship to existing minority instances. At the end of under/over sampling techniques the original dataset is changed to have a more balanced representation of classes. 

Another common method is to apply a different metric to assess the predictive power of a model. Sometimes it may be useful to select a model with lower accuracy as it provides more predictive power for specific needs of the study at hand. For instance, to a bank, from a dataset of 25633 tranactions (this is the size of the cc_train_cv datset), perhaps it is better to have a hundred or so false positives than it is to get a single false negative. False positives can be applied to further scrutiny without (significant) financial loss to the bank. Missing fraud cases (false negatives) will mean direct financial loss to the bank/account holder. This leads us to the idea of 'Cost Sensitive Learning', or CSL. In it you attempt to apply variable costs to incorrect predictions through the training process. For example, given a false negative is more of a problem for the bank than a false positive, it would have a higher cost value and thereby any effort to minimise the overall cost score will result in a smaller false negative rate, likely at the expense of the false positive rate. 

In this work I will focus on both dataset balancing methods and the application of a CSL to direct the training process to minimise false negatives. 



### 2.6.1 - SMOTE algorithm and application of Cost Sensitive Learning (CSL)
After some reading about methods to deal with data balancing, the program SMOTE seemed to stand out. Here I implemented the version from the DMwR package. 

Upon reading more about this algorithm I found out that it handle both under and over sampling. The parameter perc.under will cause a random undersampling of the data proportional to the value given. Over sampling is governed by the perc.over parameter and will generate new instances of the minority class based on a nearest neighbour approach. The k parameter will govern how many neighbours are required. 

I will begin by screening a range of perc_u and perc_o values and assess them against randomForest, looking at a range of metrics. Here I will implement my first CSL metric in an attempt to focus on the cost of getting a false negative versus a false positive. Each false positive will receive a weightest 'cost' that is different to how false positives are scored. The CSL metric is the sum of these values. In the first case, the CSL is simply 100*F_neg count plus the F_pos count. See below. 

```{r First SMOTE screen with perc_o perc_u, include=F}
perc_o <- seq(100, 1000, 100)
perc_u <- seq(100, 1000, 100)

if(!require(MLmetrics)) install.packages("MLmetrics", repos = "http://cran.us.r-project.org")
grid <- expand_grid(perc_o, perc_u)
fun1 <- function(a, b) {
  set.seed(2, sample.kind = "Rejection")
  train_smt2 <- SMOTE(Class ~ ., data = cc_train_wk, perc.over = a, k = 5, perc.under = b,
                      learner=NULL)
  set.seed(2, sample.kind = "Rejection")
  fit <- randomForest(as.matrix(train_smt2[,c(1:30)]), train_smt2$Class, ntree = 30)
  y_hat <- predict(fit, newdata = cc_train_cv[,c(1:30)])
  f1 <- F1_Score(cc_train_cv$Class, y_hat, positive = 1)
  F_pos <- table(y_hat, cc_train_cv$Class)[2,1]
  F_neg <- table(y_hat, cc_train_cv$Class)[1,2]
  CSL <- (table(y_hat, cc_train_cv$Class)[2,1] + (100*(table(y_hat, cc_train_cv$Class)[1,2])))
  Result <- c(f1, F_pos, F_neg, CSL)
}
smote_scr <- mapply(FUN = fun1, a = grid$perc_o, b = grid$perc_u)
```

Plotting the output.

```{r Plots of SMOTE screen, echo=F}
Metrics <- c("f1", "F_pos", "F_neg", "CSL")
rownames(smote_scr) <- Metrics
smote_scrT <- t(smote_scr)
smote_scrT <- as_tibble(smote_scrT) %>% mutate(perc_o = grid$perc_o, perc_u = grid$perc_u)
p1 <- smote_scrT %>% ggplot(aes(perc_u, f1)) + geom_point(aes(color = perc_o))+
  theme(legend.position = "none")
p2 <- smote_scrT %>% ggplot(aes(perc_u, F_pos)) + geom_point(aes(color = perc_o))+
  theme(legend.position = "none")
p3 <- smote_scrT %>% ggplot(aes(perc_u, F_neg)) + geom_point(aes(color = perc_o))+
  theme(legend.position = "none")
p4 <- smote_scrT %>% ggplot(aes(perc_u, CSL)) + geom_point(aes(color = perc_o))+
  theme(legend.position = "none")
grid.arrange(p1, p2, p3 + theme(legend.position = "left"), p4)
```

Looking at the plots above, f1 is highest as both perc_o and perc_u increase. Importantly false positives are lowest at high perc_u, and high perc_o but more or less level out after perc_u = 600. This means that increasing perc_o and thereby increasing oversampling reduces false positive events. Additionally, increasing perc_u keeps more of the original majority data leading to less information loss and correspondingly less false positives. False negatives are a bit more random, but lower perc_o and perc_u values seem best. A notable feature of the false negative plot is that for the occasions where F_neg was 0, there was an inverse relationship between perc_o and perc_u. 

For the CSL metric, there is not a clear pattern except perhaps that low perc_o/perc_u values gave a spike in CSL value. 

The take away from this screen is a low to middle perc_o value (dark blue) and moderate perc_u value of around 600 seems to give the lowest CSL values, and a respectable F_pos. I am choosing to ignore the f1 metric at this time as it does not seem to reflect my goal of a low F_neg and moderate at worst F_pos. 

In an effort to look at the CSL metric and how best to weight the F_neg and F_pos events, I will perform a screen of weight combinations. The objective here is to establish a metric that is robust and able to clearly give an indication of the best model. Minimising F_neg to as close to 0 as possible, and F_pos to less than 100. 

```{r CSL weighting screen, echo=F}
wt_met_scr <- smote_scrT %>% mutate(wt50 = F_pos + 50*F_neg,
                                 wt200 = F_pos + 200*F_neg,
                                 wt300 = F_pos + 300*F_neg,
                                 wt400 = F_pos + 400*F_neg,
                                 wt2_100 = 2*F_pos + 100*F_neg,
                                 wt2_200 = 2*F_pos + 400*F_neg,
                                 wt2_300 = 2*F_pos + 300*F_neg,
                                 wt2_400 = 2*F_pos + 400*F_neg)
p5 <- wt_met_scr %>% ggplot(aes(perc_u, wt50)) + geom_point(aes(color = perc_o))+
  theme(legend.position = "none") 
p6 <- wt_met_scr %>% ggplot(aes(perc_u, wt200)) + geom_point(aes(color = perc_o))+
  theme(legend.position = "none") 
p7 <- wt_met_scr %>% ggplot(aes(perc_u, wt300)) + geom_point(aes(color = perc_o))+
  theme(legend.position = "none") 
p8 <- wt_met_scr %>% ggplot(aes(perc_u, wt400)) + geom_point(aes(color = perc_o))+
  theme(legend.position = "none") 
p9 <- wt_met_scr %>% ggplot(aes(perc_u, wt2_100)) + geom_point(aes(color = perc_o))+
  theme(legend.position = "none") 
p10 <- wt_met_scr %>% ggplot(aes(perc_u, wt2_200)) + geom_point(aes(color = perc_o))+
  theme(legend.position = "none") 
p11 <- wt_met_scr %>% ggplot(aes(perc_u, wt2_300)) + geom_point(aes(color = perc_o))+
  theme(legend.position = "none") 
p12 <- wt_met_scr %>% ggplot(aes(perc_u, wt2_400)) + geom_point(aes(color = perc_o))+
  theme(legend.position = "none") 
grid.arrange(p5, p4, p6, p7, p8, p9, p10, p11, p12)
```

Table showing how CSL weighted metrics are calculated. 

```{r, echo=F}
tibble(Metric = c("wt50", "wt100", "CSL", "wt300", "wt400", "wt2_100", "wt2_200", "wt2_300", "wt2_400"),
       XF_pos = c(1,1,1,1,1,2,2,2,2), XF_neg = c(50, 100, 200, 300, 400, 100, 200, 300, 400)) %>% 
  knitr::kable()
```

The table above outlines how the metrics are calculated, being a sum of XF_pos (a number multiplied by F_pos count) and XF_neg (a number multiplied by F_neg). I think it is fair to say the spread of the data is optimal for wt300 to wt400. Multiplying the F_pos by 2 made little difference to the spread. 

Interestingly there is an inverted pattered where for low perc_u, a high perc_o gives lower CSL score. This then inverts at perc_u = 600 where now a low perc_o value returns the lower CSL values. 

There are two clear 'troughs' above displaying low CSL scores for perc_u = 400, and perc_u = 700. The following screen is in fact a screen withing a screen. For each cycle of the perc_o/perc_u screen I implement a seed screen to test the first ten seeds. This is my attempt to control the randomness of the SMOTE/randomForest system (see below in section 2.6.1.1 for more details). 

Below is a finer screen of the perc_o/perc_u combinations with two runs. One is high perc_u with low perc_o, and the other is inverted from that with low perc_u with high perc_o. This screen takes some time but it is useful. 

```{r SMOTE fine screen, include=F}
# This takes some time.
perc_oL <- seq(100, 400, 50)
perc_uH <- seq(500, 700, 50)
perc_oH <- seq(500, 700, 50)
perc_uL <- seq(100, 400, 50)
grid2 <- expand_grid(perc_oL, perc_uH)
grid3 <- expand_grid(perc_oH, perc_uL)
fun2 <- function(a, b) {
  seeds <- seq(1,10,1)
  best_seed <- sapply(seeds, function(x){
    set.seed(x, sample.kind = "Rejection")
    train_smt2 <- SMOTE(Class ~ ., data = cc_train_wk, perc.over = a, k = 5, perc.under = b,
                      learner=NULL)
    set.seed(2, sample.kind = "Rejection")
    fit <- randomForest(as.matrix(train_smt2[,c(1:30)]), train_smt2$Class, ntree = 30)
    y_hat <- predict(fit, newdata = cc_train_cv[,c(1:30)])
    CSL <- (table(y_hat, cc_train_cv$Class)[2,1] + (400*(table(y_hat, cc_train_cv$Class)[1,2])))
    CSL
  })
  set.seed(seeds[which.min(best_seed)], sample.kind = "Rejection")
  train_smt2 <- SMOTE(Class ~ ., data = cc_train_wk, perc.over = a, k = 5, perc.under = b,
                      learner=NULL)
  set.seed(2, sample.kind = "Rejection")
  fit <- randomForest(as.matrix(train_smt2[,c(1:30)]), train_smt2$Class, ntree = 30)
  y_hat <- predict(fit, newdata = cc_train_cv[,c(1:30)])
  f1 <- F1_Score(cc_train_cv$Class, y_hat, positive = 1)
  F_pos <- table(y_hat, cc_train_cv$Class)[2,1]
  F_neg <- table(y_hat, cc_train_cv$Class)[1,2]
  CSL <- (table(y_hat, cc_train_cv$Class)[2,1] + (400*(table(y_hat, cc_train_cv$Class)[1,2])))
  result <- c(f1, F_pos, F_neg, CSL)
}

smote_hi_U <- mapply(FUN = fun2, a = grid2$perc_oL, b = grid2$perc_uH)
smote_low_U <- mapply(FUN = fun2, a = grid3$perc_oH, b = grid3$perc_uL)
```


The table below highlights the top ten highest CSL (wt400) scores for a Low perc_o, High perc_u combinations. 

```{r SMOTE fine screen table1, echo=F}
smote_hi_UT <- t(smote_hi_U)
colnames(smote_hi_UT) <- c("f1", "F_pos", "F_neg", "wt400")
smote_hi_UT <- as_tibble(smote_hi_UT) %>% mutate(perc_o = grid2$perc_oL, perc_u = grid2$perc_uH)
tmp <- smote_hi_UT %>% arrange(wt400)
head(tmp,10) %>% knitr::kable(caption = "Low perc_o, High perc_u")
rm(tmp)
```

The table below highlights the top ten highest CSL (wt400) scores for a High perc_o, Low perc_u combinations. 

```{r SMOTE fine screen table2, echo=F}
smote_low_UT <- t(smote_low_U)
colnames(smote_low_UT) <- c("f1", "F_pos", "F_neg", "wt400")
smote_low_UT <- as_tibble(smote_low_UT) %>% mutate(perc_o = grid3$perc_oH, perc_u = grid3$perc_uL)
tmp <- smote_low_UT %>% arrange(wt400)
head(tmp,10) %>% knitr::kable()
rm(tmp)
```

In both tables shown just above, I showcase some improved results from the 'benchmark' regression work above. The best results give F_neg values at zero in both tables, but the F_pos count is higher than we have see at >30. The best answer is clearly perc_o = 400/perc_u = 700, with F_neg of 0, and F_pos of a modest 33. I will focus on using this moving forward. 

#### 2.6.1.1 - Sensitivity to tree number. 
It is worth noting that after some extensive trial and error in applying various screens, I have noticed something interesting about this problem. The outcomes are very unstable. Often giving considerably different results each time it is run. This stems from the random nature of two of its core processes, and also to the fact that it is an imbalanced dataset. Obviously there is a randomness to the SMOTE algorithm as majority classes are randomly undersampled. With a dataset as skewed in favour of the majority class like this one is, it is not surprinsing that models vary from run to run, having some that are 'fortunate' in picking instances that are 'strong performers' when it comes to the random forest, and some that are less 'fortunate'. In addition to the SMOTE algorithm, there is of course the randomForest implementation itself, as there is a significantly random element to its function as the name would suggest. 

I have managed to control the random aspect of my runs by setting seeds before each so I can predict the outcome more. I have in fact performed screens of seed number using CSL as a metric to decide which seed gives the best model (see below). What I found interesting however, is that the models are also very susceptible to the number of trees (ntree) parameter of randomForest. Though not that surprising as you would expect more trees would give the more robust and reproducible model. This may be the case still, but in my hands the best models came from instances of low tree numbers (typically 30 per run). The way I would attempt to explain this is that in the case of fewer trees, the impact of a few 'strong performers' will be less diluted out by averaging. Equally the 'poor performers' will have a more pronounced effect. As such through the careful use of seeding I am able to screen a few models and pick one that is a good performer. 

NOTE: In this work I am using set.seed with sample.kind = "Rejection". If you are not using this method of sampling then the results will differ for you. 

Below I am performing a seed screen on part of the original result SMOTE screen from above. I am attempting to repeat the outcome of the fine perc_o/perc_u screen above that yielded 0 F_neg, and 33 F_pos. I am using the CSL score to evaluate the outcome. 

```{r Seed screen of best SMOTE outcome, include=F}
seeds <- seq(1,10,1)
best_seed <- sapply(seeds, function(x){
  set.seed(x, sample.kind = "Rejection")
  train_smt2 <- SMOTE(Class ~ ., data = cc_train_wk, perc.over = 400, k = 5, perc.under = 700,
                    learner=NULL)
  set.seed(2, sample.kind = "Rejection")
  fit <- randomForest(as.matrix(train_smt2[,c(1:30)]), train_smt2$Class, ntree = 30)
  y_hat <- predict(fit, newdata = cc_train_cv[,c(1:30)])
  CSL <- (table(y_hat, cc_train_cv$Class)[2,1] + (400*(table(y_hat, cc_train_cv$Class)[1,2])))
  CSL
})
```

```{r Best seed result}
# Seed that gives best result;
seeds[which.min(best_seed)]
```


Now to apply this seed.set to a screen of ntree parameter.

```{r nTree screen, include=F}
### Looking at ntree
ntrees <- c(20, 30, 50, 100, 500)
NT_scr1 <- sapply(ntrees, function(x) {
  set.seed(6, sample.kind = "Rejection")
  train_smt2 <- SMOTE(Class ~ ., data = cc_train_wk, perc.over = 400, k = 5, perc.under = 700,
                      learner=NULL)
  set.seed(2, sample.kind = "Rejection")
  fit <- randomForest(as.matrix(train_smt2[,c(1:30)]), train_smt2$Class, ntree = x)
  y_hat <- predict(fit, newdata = cc_train_cv[,c(1:30)])
  f1 <- F1_Score(cc_train_cv$Class, y_hat, positive = 1)
  F_pos <- table(y_hat, cc_train_cv$Class)[2,1]
  F_neg <- table(y_hat, cc_train_cv$Class)[1,2]
  CSL <- (table(y_hat, cc_train_cv$Class)[2,1] + (400*(table(y_hat, cc_train_cv$Class)[1,2])))
  Result <- c(f1, F_pos, F_neg, CSL)
  Result
})
```

```{r Table from nTree screen, echo=F}
NT_scr1T <- (t(NT_scr1))
NT_scr1T <- as_tibble(NT_scr1T)
colnames(NT_scr1T) <- c("f1", "F_pos", "F_neg", "wt400")
NT_scr1T <- NT_scr1T %>% mutate(NT = ntrees)
NT_scr1T %>% arrange(wt400) %>% knitr::kable()
```

It is clear from the above table that there is a notable difference in outcome as the number of trees (NT) vary. Ranked by wt400 score, a tree count of 30 is optimal. For all values higher, a fraud case is missed in the resulting model. Without getting into a theoritical rational for this, it is clear I can reproducted the outcome of 0 F_neg and 33 F_pos by contraining ntree to 30. This is all that matters. 


### 2.6.2 - Settling on the best SMOTE dataset
Having now established the best SMOTE modified dataset, I can confirm the best seed set for the random forest run itself. 

```{r Making final SMOTE dataset, include=F}
# Preparing final SMOTE dataset
set.seed(6, sample.kind = "Rejection")
smt_final <- SMOTE(Class ~ ., data = cc_train_wk, perc.over = 400, k = 5, perc.under = 700,
                    learner=NULL)
```

Looking briefly at the output dataset. 

```{r, echo=F}
# Class breakdown. 
SMT_sum <- tibble(Dataset = c("cc_train_wk", "cc_train_wk", "smt_final", "smt_final"),
       Class = c(0, 1, 0, 1),
       Count = c(summary(cc_train_wk$Class), summary(smt_final$Class)),
       Total_instances = c(length(cc_train_wk$Class), length(cc_train_wk$Class),
                           length(smt_final$Class), length(smt_final$Class)))
SMT_sum %>% knitr::kable()
```

You can see from the summary table that total instances are dramatically reduced, and while there is still some imbalance in the data, it is not as extreme as in the original data. 

#### 2.6.2.1 Screening the best randomForest seed. 

```{r Best randomforest seed screen, include=F}
# Seed testing
seeds <- seq(1,10,1)
seed_scr1 <- sapply(seeds, function(x) {
  set.seed(x, sample.kind = "Rejection")
  fit <- randomForest(as.matrix(smt_final[,c(1:30)]), smt_final$Class, ntree = 30, mtry = 5)
  y_hat <- predict(fit, newdata = cc_train_cv[,c(1:30)])
  CSL <- (table(y_hat, cc_train_cv$Class)[2,1] + (400*(table(y_hat, cc_train_cv$Class)[1,2])))
  CSL
})
```

```{r}
seeds[which.min(seed_scr1)]
```

Not surprisingly the best seed for the randomForest algorithm is again 2, the same as was applied when looking at the best seed for the SMOTE algorithm. It seems the random event that yields a F_neg of 0 and F_pos of 33 is highly prone to parameterisation. For the record I also screened the SMOTE k parameter, and randomForest mtry parameter. In each case the best arrangement are the (default) values for these parameters that I applied when seed screening above. These were; k = 5, and mtry = 5. 


### 2.6.3 - Rerunning Rborist model with best SMOTE dataset
For piece of mind, reruning the original Rborist model on this new SMOTE data gives the following results;

```{r}
set.seed(1, sample.kind = "Rejection")
rf_2 <- Rborist(as.matrix(smt_final[,c(1:30)]), smt_final$Class, predFixed = 20, minNode = 15)
rf_pred2 <- predict(rf_2, newdata = cc_train_cv[,c(1:30)])
rf_tab2 <- table(rf_pred2$yPred, cc_train_cv$Class)
rownames(rf_tab2) <- c("Pred 0", "Pred 1")
colnames(rf_tab2) <- c("Actual 0", "Actual 1")
rf_tab2 %>% knitr::kable()
```

While this improves on F_neg by predicting one more fraud case, it still misses one, and F_pos jumps up to 82. 



This is now ready to be applied to a final model. 

\newpage
# 3 - Results
## 3.1 - Benchmark model
This work began with an effort to create a simple benchmark model that will be used for comparison for all subsequent models. 

Basic linear regression using all predictors gave some already decent statistics with and Accuracy of 0.99957, RMSE of 0.0207, false positives (F_pos) of 5, and false negative (F_neg) of 2. In total there were only 39 fraud cases in total in the cc_train_cv dataset (against which all models are tested during training), out of 22633 total transactions. Considering this, missing 2 out of 39 is 5.1% of all fraudulent instances. This will need to be improved. 

An attempt was made to reduce potential overtraining by limiting predictors to those that had demonstrable correlation with the response variable (Class). This had no effect. 

Logistic regression was also tried by did not result in an improved model. Linear regression using all predictors was adopted as the benchmark model.


## 3.2 - Model development
### 3.2.1 Rborist Random Forest
The first model attempted post regression work was a random forest. Rborist was used as the cc_train_wk dataset was quite large (259174 instances). After some screening of key parameters, the resulting model (RF Rborist) performed better than the benchmark lm model with still 2 F_neg, but the F_post count was reduced to 1. Given the focus of this work is on minimising the F_neg, this is not a a significant improvement over the benchmark model. It was also incredibly computationally demanding so other options were sought. 

### 3.2.2 SMOTE work and application of CSL metric
Attention was now paid to the imbalanced nature of this dataset. Dataset balancing operations were undertaken and a Cost Sensitive Learning (CSL) metric was employed to take the focus away from metrics such as accuracy and f1. This new CSL metric brings focus to the high importance of minimising F_neg values. 

The SMOTE algorithm was determined to have the best effect on CSL values. It was determined that the resulting SMOTE datasets were highly susceptible to chance. Setting the seed became critical to success and seed screens were implemented. Once seeds were optimised and applied, further screening of parameters proved futile as the optimal CSL values were always associated with default parameters run in the seed screen. A final SMOTE dataset and corresponding best parameter sets for the subsequent randomForest algorithm, were determined.


### 3.2.3 Rerun Rborist against SMOTE dataset
Just as a means of being thorough, the original Rborist run that was performed against the full cc_train_wk dataset, was rerun against the new SMOTE dataset. An improvement came in the the reduction of the F_neg from 2 to 1. This is not as good as what I saw with the randomForest algorithm. Notably the F_pos count was 53. 


### 3.2.4 Final model (SMOTE/randomForest)
Applying this to a final model.
```{r Final randomforest model, include=F}
set.seed(2, sample.kind = "Rejection")
fit_RF <- randomForest(as.matrix(smt_final[,c(1:30)]), smt_final$Class, ntree = 30, mtry = 5)
y_hat <- predict(fit_RF, newdata = cc_train_cv[,c(1:30)])
f1 <- F1_Score(cc_train_cv$Class, y_hat, positive = 1)
F_pos <- table(y_hat, cc_train_cv$Class)[2,1]
F_neg <- table(y_hat, cc_train_cv$Class)[1,2]
CSLf <- (table(y_hat, cc_train_cv$Class)[2,1] + (400*(table(y_hat, cc_train_cv$Class)[1,2])))
Result <- c(f1, F_pos, F_neg, CSLf)
```




```{r, echo=F}
model_results2 <- model_results %>% mutate(CSL = c(lm_tab[2,1] + (400*lm_tab[1,2]),
                                                 lm_tab2[2,1] + (400*lm_tab2[1,2]),
                                                 log_tab[2,1] + (400*log_tab[1,2]),
                                                 rf_tab1[2,1] + (400*rf_tab1[1,2])))
model_results2 <- bind_rows(model_results2,
                          data_frame(method = "Rborist with SMOTE",
                                     Accuracy = mean(rf_pred2$yPred == cc_train_cv$Class),
                                     false_pos = rf_tab2[2,1], 
                                     false_neg = rf_tab2[1,2],
                                     CSL = rf_tab2[2,1] + (400*rf_tab2[1,2])))
model_results2 <- bind_rows(model_results2,
                          data_frame(method = "Final Model vs cc_train_cv",
                                     Accuracy = mean(y_hat == cc_train_cv$Class),
                                     false_pos = F_pos, 
                                     false_neg = F_neg,
                                     CSL = CSLf))
model_results2 %>% knitr::kable()
```

Application of SMOTE dataset balancing and the use of a CSL metric was able to drop the false negatives by one. The false positive count is higher but still moderate. Using the evaluation of F_neg importance reached in this work, the CSL score is improved from 2006 to 1629. 


## 3.3 - Final Model Tested Against cc_test

Applying this final model against the cc_test testing dataset.

```{r}
y_hat_t <- predict(fit_RF, newdata = cc_test[,c(1:30)])
f1_t <- F1_Score(cc_test$Class, y_hat_t, positive = 1)
F_pos_t <- table(y_hat_t, cc_test$Class)[2,1]
F_neg_t <- table(y_hat_t, cc_test$Class)[1,2]
CSL_t <- (table(y_hat_t, cc_test$Class)[2,1] + (400*(table(y_hat_t, cc_test$Class)[1,2])))
Result_test <- c(f1_t, F_pos_t, F_neg_t, CSL_t)
```

```{r, echo=F}
model_results2 <- bind_rows(model_results2,
                          data_frame(method = "Final Model vs cc_test",
                                     Accuracy = mean(y_hat_t == cc_test$Class),
                                     false_pos = F_pos_t, 
                                     false_neg = F_neg_t, CSL = CSL_t))
model_results2 %>% knitr::kable()
```

In quite a remarkable outcome, the model predicts all fraud cases, and only miss-predicts 33 non-fraud cases as frauds. The CSL score is only 33. This will be discussed further below.


\newpage
# 4 - Conclusions
This body of work is structured to show a process of discovery by the author. Given the significant amount of work done on this data set that is available online, every effort has been made to avoid sampling the work of others. A consequence of this is I may have missed an obvious treatment of the problem that is long established in the Data Science community. 

I began by treating the Creditcard Fraud dataset (ccds) as a normal, though large dataset. Though self explanatory descriptors like Time and Amount were included in the dataset, the vast majority of the descriptor content of the dataset was the outcome of a Principle Component Analysis. The original descriptors from which these 28 PC descriptors are derived from were intentionally left out of the dataset for privacy reasons. These PCs however are still powerful instructors for the development of a classification algorithm. 

Treating the dataset first with a regression approach, I found that linear regression applied to all predictors was the most effective model and I set this as the benchmark. Of the 39 cases of fraud in the cc_train_cv dataset (out of 25633 total instances), the benchmark model was able to predict all but 2 of these cases. This is 5.1% of the number of fraud cases. The false positive count was only 5. 

I then addressed the elephant in the room, which was that fact the dataset is incredibly imbalanced. I  used SMOTE algorithm to balance out the dataset, which I sourced from the DMwR package. At this time I also implemented a new metric of my own creation to assess model success. It was intended to be a Cost Sensitive Learning (CSL) metric where I treat the instance of a false negative particularly 'negatively'. In this way I found that multiplying the F_pos count by 1, and the F_neg count by 400, then summing them gave the best means to driving model success. This approach was guided by my own assumption that the bank(s) would be far more interested in finding all cases of fraud, and less concerned about the classification system flagging non-fraud cases as fraud. The latter situation could be further scrutinised with minimal effort, so long as their count is not too high. 

As the SMOTE algorithm served to reduce the size of the dataset, randomForest was used in preference to Rborist as the computational requirements were reduced and I prefer the output of randomForest. Cycles of screening and playing around with the parameters of the SMOTE and randomForest algorithms made me aware of the randomness of both methods of analysis. I began systematically applying seeds to take control of the reproducibility of my output. I ended up settling on a clear set of parameters that resulted in classification model that reported all of the 39 fraud cases. This is down to 0%, from 5.5% from the benchmark model. There was in increase in F_pos to 33 incorrectly flagged instances. This is only a moderate amount to apply to further scrutiny. Arguably this is not much of an improvement over the basic linear regression approach. The answer to that argument will by how much value do you place on catching that extra case of fraud. When this final model was applied to the set aside cc_test data, importantly it also predicted all cases of fraud, and had only 33 cases of false positives. Assuming the importance of false negaves over false positives, this is a significant improvement, dropping from 2 to 0.  

## 4.1 - Future Directions
The final model presented in this report was completely successful in predicting all cases of fraud in the cc_test testing dataset. This method would be valuable to any banking institution assuming they could recreate the Principle Component Analysis that generated most of the predictors used in the model. One obvious are of improvement for future development of the model is to bring down the false positive count. It is the opinion of the author that it is not a drama to have 33 out of 25633 instances as incorrectly being flagged as fraud, because this small number can be further scrutinised more manually. However the model would obvioulsy benefit from reporting fewer false positives. An investigation into what makes this so system so prone to random variability would likely help. By that I mean to pay attention to models that return the best and worst stats in one of my seed screens and report on differences in the feature importance for the random forest models associated with each. This may give some important insight to the key features/predictors that are driving the classification of the "difficult" cases of fraud. Paying more attention to these may facilitate fine tuning of the model to result in classification of all fraud cases, but a reduction too in false positives. 






